{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c0e3691a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = 'C:\\\\Users\\\\PAPASOFT INC\\\\Documents\\\\GitHub\\\\Soft-Silhouette\\\\Results\\\\emnist_mnist\\\\DCNAE\\\\100_Eps_ld_10_out_10_bs_256_lr_0.0005_lamda_1_beta_1e-05\\\\Data.xlsx'\n",
    "#file_path = 'C:\\\\Users\\\\PAPASOFT INC\\\\Documents\\\\GitHub\\\\Soft-Silhouette\\\\Results\\\\emnist_balanced_digits\\\\SSAE\\\\100_Eps_ld_10_out_10_bs_256_lr_0.0005_sil_lambda_0.01_entr_lambda_0.01\\\\Data.xlsx'\n",
    "xls = pd.ExcelFile(file_path)\n",
    "\n",
    "# Create an empty list to store DataFrames for each sheet\n",
    "data_frames = []\n",
    "\n",
    "# Loop through the sheets and read them into DataFrames\n",
    "for sheet_name in xls.sheet_names[:]:\n",
    "    df = pd.read_excel(xls, sheet_name, skiprows=[i for i in range(1,100)], engine='openpyxl')\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "    data_frames.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "combined_df = pd.concat(data_frames)\n",
    "\n",
    "# Calculate statistics\n",
    "statistics = combined_df.describe().round(2)\n",
    "\n",
    "# Create a Pandas Excel writer using openpyxl as the engine\n",
    "with pd.ExcelWriter(file_path, engine='openpyxl', mode='a') as writer:\n",
    "    # Write the statistics to a new sheet named 'Statistics'\n",
    "    statistics.to_excel(writer, sheet_name='Stats', index=True)\n",
    "\n",
    "xls.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa9c1993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.7849857\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a list of numbers\n",
    "numbers = [0.770929, 0.772786, 0.787821, 0.793286, 0.798071, 0.817893, 0.787714, 0.776857, 0.761929, 0.782571]\n",
    "\n",
    "# Calculate the mean of the numbers using numpy\n",
    "mean = np.mean(numbers)\n",
    "\n",
    "# Print the mean\n",
    "print(\"Mean:\", mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29e44584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(data['x_train'])=7769\n",
      "data['x_train'].shape=(7769, 26147)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "Utility file for the Reuters text categorization benchmark dataset.\n",
    "\n",
    "See also\n",
    "--------\n",
    "http://www.vision.caltech.edu/Image_Datasets/Caltech101/\n",
    "\"\"\"\n",
    "\n",
    "from nltk.corpus import reuters, stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "n_classes = 90\n",
    "labels = reuters.categories()\n",
    "\n",
    "\n",
    "def load_data(config={}):\n",
    "    \"\"\"\n",
    "    Load the Reuters dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data : dict\n",
    "        with keys 'x_train', 'x_test', 'y_train', 'y_test', 'labels'\n",
    "    \"\"\"\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "    mlb = MultiLabelBinarizer()\n",
    "\n",
    "    documents = reuters.fileids()\n",
    "    test = [d for d in documents if d.startswith('test/')]\n",
    "    train = [d for d in documents if d.startswith('training/')]\n",
    "\n",
    "    docs = {}\n",
    "    docs['train'] = [reuters.raw(doc_id) for doc_id in train]\n",
    "    docs['test'] = [reuters.raw(doc_id) for doc_id in test]\n",
    "    xs = {'train': [], 'test': []}\n",
    "    xs['train'] = vectorizer.fit_transform(docs['train']).toarray()\n",
    "    xs['test'] = vectorizer.transform(docs['test']).toarray()\n",
    "    ys = {'train': [], 'test': []}\n",
    "    ys['train'] = mlb.fit_transform([reuters.categories(doc_id)\n",
    "                                     for doc_id in train])\n",
    "    ys['test'] = mlb.transform([reuters.categories(doc_id)\n",
    "                                for doc_id in test])\n",
    "    data = {'x_train': xs['train'], 'y_train': ys['train'],\n",
    "            'x_test': xs['test'], 'y_test': ys['test'],\n",
    "            'labels': globals()[\"labels\"]}\n",
    "    return data\n",
    "\n",
    "config = {}\n",
    "data = load_data(config)\n",
    "print(\"len(data['x_train'])={}\".format(len(data['x_train'])))\n",
    "print(\"data['x_train'].shape={}\".format(data['x_train'].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46e895a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_rcv1\n",
    "rcv1 = fetch_rcv1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1ad7dae",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sparse matrix length is ambiguous; use getnnz() or shape[0]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m         docs_with_multiple_categories\u001b[38;5;241m.\u001b[39mappend(i)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Remove documents with multiple categories\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m filtered_indices \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrcv1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m docs_with_multiple_categories]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Create a new dataset with single-category documents\u001b[39;00m\n\u001b[0;32m     17\u001b[0m single_category_data \u001b[38;5;241m=\u001b[39m [rcv1\u001b[38;5;241m.\u001b[39mdata[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m filtered_indices]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py:345\u001b[0m, in \u001b[0;36mspmatrix.__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 345\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix length is ambiguous; use getnnz()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    346\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or shape[0]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: sparse matrix length is ambiguous; use getnnz() or shape[0]"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_rcv1\n",
    "import numpy as np\n",
    "\n",
    "# Load the RCV1 dataset\n",
    "rcv1 = fetch_rcv1(subset='all')\n",
    "\n",
    "# Determine the indices of documents with multiple categories\n",
    "docs_with_multiple_categories = []\n",
    "for i, target in enumerate(rcv1.target):\n",
    "    if np.sum(target) > 1:\n",
    "        docs_with_multiple_categories.append(i)\n",
    "\n",
    "# Remove documents with multiple categories\n",
    "filtered_indices = [i for i in range(len(rcv1.data)) if i not in docs_with_multiple_categories]\n",
    "\n",
    "# Create a new dataset with single-category documents\n",
    "single_category_data = [rcv1.data[i] for i in filtered_indices]\n",
    "single_category_target = [rcv1.target[i] for i in filtered_indices]\n",
    "\n",
    "# Filter the data based on your selected categories\n",
    "selected_categories = [\"CCAT\", \"ECAT\", \"GCAT\", \"MCAT\", \"GCAT\", \"GCAT\"]\n",
    "filtered_indices = []\n",
    "for i, target in enumerate(rcv1.target):\n",
    "    if any(category in target for category in selected_categories):\n",
    "        filtered_indices.append(i)\n",
    "\n",
    "X = rcv1.data[filtered_indices]\n",
    "\n",
    "# Preprocess the data (e.g., TF-IDF vectorization)\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b899472",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "import scipy.io\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.datasets import make_circles, make_moons, make_blobs, fetch_20newsgroups, fetch_openml\n",
    "from mnist import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from PIL import Image\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, reuters\n",
    "\n",
    "np.unique(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f9f3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "import scipy.io\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.datasets import make_circles, make_moons, make_blobs, fetch_20newsgroups, fetch_openml\n",
    "from mnist import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from PIL import Image\n",
    "from sklearn.datasets import fetch_rcv1\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, reuters\n",
    "folder_path = './Datasets/'\n",
    "\n",
    "df_train = pd.read_csv(folder_path + 'HARS/train.csv', index_col=0)\n",
    "df_test = pd.read_csv(folder_path + 'HARS/test.csv', index_col=0)\n",
    "df = pd.concat([df_train, df_test], axis=0, ignore_index=True)\n",
    "for i in df_train.columns:\n",
    "    print(i)\n",
    "del df_train, df_test\n",
    "\n",
    "labels = df['Activity']\n",
    "df.drop(columns=['subject','Activity'], inplace=True)\n",
    "data = np.array(df)\n",
    "labels = np.squeeze(np.array(labels))\n",
    "\n",
    "data = MinMaxScaler().fit_transform(data)\n",
    "labels = LabelEncoder().fit_transform(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33651b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def histogram_equalization(image_path):\n",
    "    # Open the image\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    img_gray = img.convert('L')\n",
    "\n",
    "    # Get image data as a NumPy array\n",
    "    img_data = np.array(img_gray)\n",
    "\n",
    "    # Calculate the histogram\n",
    "    histogram = np.histogram(img_data, bins=256, range=(0, 256), density=True)\n",
    "\n",
    "    # Calculate the cumulative distribution function (CDF)\n",
    "    cdf = histogram[0].cumsum()\n",
    "\n",
    "    # Apply histogram equalization\n",
    "    equalized_image_data = (cdf[img_data] * 255).astype(np.uint8)\n",
    "\n",
    "    # Create a new image from the equalized data\n",
    "    equalized_image = Image.fromarray(equalized_image_data)\n",
    "\n",
    "    return equalized_image\n",
    "\n",
    "def plot_histogram(image_data, title):\n",
    "    histogram = np.histogram(image_data, bins=256, range=(0, 256), density=True)\n",
    "    plt.figure()\n",
    "    plt.hist(image_data.ravel(), bins=256, range=(0, 256), density=True, color='gray', alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Pixel Value')\n",
    "    plt.ylabel('Normalized Frequency')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Process image 2.bmp\n",
    "equalized_image_2 = histogram_equalization('2.bmp')\n",
    "original_image_2 = Image.open('2.bmp')\n",
    "original_image_data_2 = np.array(original_image_2)\n",
    "\n",
    "plot_histogram(original_image_data_2, 'Original Histogram (Image 2)')\n",
    "plot_histogram(np.array(equalized_image_2), 'Equalized Histogram (Image 2)')\n",
    "equalized_image_2.show()\n",
    "\n",
    "# Process image 4.bmp\n",
    "equalized_image_4 = histogram_equalization('4.bmp')\n",
    "original_image_4 = Image.open('4.bmp')\n",
    "original_image_data_4 = np.array(original_image_4)\n",
    "\n",
    "plot_histogram(original_image_data_4, 'Original Histogram (Image 4)')\n",
    "plot_histogram(np.array(equalized_image_4), 'Equalized Histogram (Image 4)')\n",
    "equalized_image_4.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b6dbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def histogram_equalization(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Apply histogram equalization using OpenCV\n",
    "    equalized_image = cv2.equalizeHist(image)\n",
    "\n",
    "    return equalized_image\n",
    "\n",
    "def plot_histogram(image_data, title):\n",
    "    histogram = cv2.calcHist([image_data], [0], None, [256], [0, 256])\n",
    "    histogram = histogram.ravel() / histogram.sum()\n",
    "    cdf = histogram.cumsum()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(image_data.ravel(), bins=256, range=(0, 256), density=True, color='gray', alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Pixel Value')\n",
    "    plt.ylabel('Normalized Frequency')\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(cdf, color='b')\n",
    "    plt.title(title + ' CDF')\n",
    "    plt.xlabel('Pixel Value')\n",
    "    plt.ylabel('CDF Value')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Process image 2.bmp\n",
    "equalized_image_2 = histogram_equalization('2.bmp')\n",
    "plot_histogram(cv2.imread('2.bmp', cv2.IMREAD_GRAYSCALE), 'Original Histogram (Image 2)')\n",
    "plot_histogram(equalized_image_2, 'Equalized Histogram (Image 2)')\n",
    "\n",
    "# Process image 4.bmp\n",
    "equalized_image_4 = histogram_equalization('4.bmp')\n",
    "plot_histogram(cv2.imread('4.bmp', cv2.IMREAD_GRAYSCALE), 'Original Histogram (Image 4)')\n",
    "plot_histogram(equalized_image_4, 'Equalized Histogram (Image 4)')\n",
    "\n",
    "# Display the equalized images\n",
    "cv2.imshow('Equalized Image (Image 2)', equalized_image_2)\n",
    "cv2.imshow('Equalized Image (Image 4)', equalized_image_4)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4281ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the image\n",
    "image_path = 'input_image.jpg'\n",
    "image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Method 1: Histogram equalization using cv2.equalizeHist\n",
    "equalized_image_cv2 = cv2.equalizeHist(image)\n",
    "\n",
    "# Method 2: Manual histogram equalization\n",
    "def manual_histogram_equalization(image_path):\n",
    "    # Load the image\n",
    "    img = Image.open(image_path)\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    img_gray = img.convert('L')\n",
    "\n",
    "    # Get image data as a NumPy array\n",
    "    img_data = np.array(img_gray)\n",
    "\n",
    "    # Calculate the histogram\n",
    "    histogram = np.histogram(img_data, bins=256, range=(0, 256), density=True)\n",
    "\n",
    "    # Calculate the cumulative distribution function (CDF)\n",
    "    cdf = histogram[0].cumsum()\n",
    "\n",
    "    # Apply histogram equalization\n",
    "    equalized_image_data = (cdf[img_data] * 255).astype(np.uint8)\n",
    "\n",
    "    # Create a new image from the equalized data\n",
    "    equalized_image = Image.fromarray(equalized_image_data)\n",
    "\n",
    "    return equalized_image\n",
    "\n",
    "equalized_image_manual = manual_histogram_equalization(image_path)\n",
    "\n",
    "# Display original and equalized images\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Equalized Image (cv2)', equalized_image_cv2)\n",
    "equalized_image_manual.show()\n",
    "\n",
    "# Plot histograms\n",
    "def plot_histogram(image_data, title):\n",
    "    histogram = np.histogram(image_data, bins=256, range=(0, 256), density=True)\n",
    "    histogram = histogram[0].ravel() / histogram[0].sum()\n",
    "    cdf = histogram.cumsum()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(image_data.ravel(), bins=256, range=(0, 256), density=True, color='gray', alpha=0.7)\n",
    "    plt.title(title + ' Histogram')\n",
    "    plt.xlabel('Pixel Value')\n",
    "    plt.ylabel('Normalized Frequency')\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(cdf, color='b')\n",
    "    plt.title(title + ' CDF')\n",
    "    plt.xlabel('Pixel Value')\n",
    "    plt.ylabel('CDF Value')\n",
    "    plt.grid()\n",
    "\n",
    "# Plot histograms for both methods\n",
    "plot_histogram(image, 'Original')\n",
    "plot_histogram(equalized_image_cv2, 'Equalized (cv2)')\n",
    "plot_histogram(np.array(equalized_image_manual), 'Equalized (Manual)')\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
