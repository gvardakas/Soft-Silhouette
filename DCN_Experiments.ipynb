{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2ce3775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from Related_Codes.DCNAutoencoder import DCNAutoencoder, DCNCDAutoencoder\n",
    "from Related_Codes.DECAutoencoder import DECAutoencoder, DECCDAutoencoder\n",
    "from Related_Codes.IDECAutoencoder import IDECAutoencoder, IDECCDAutoencoder\n",
    "from General_Functions import General_Functions\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, NMF\n",
    "from sklearn.manifold import LocallyLinearEmbedding, SpectralEmbedding, Isomap, TSNE\n",
    "\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from Datasets.Datasets_Functions import *\n",
    "from Visualization import Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6287da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_module = './'\n",
    "sys.path.append(path_to_module)\n",
    "os.environ['OMP_NUM_THREADS'] = '6'\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10c7919d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tcga': {'batch_size': 256, 'n_clusters': 5, 'module_name': 'Datasets.Datasets'}, 'emnist_balanced_letters': {'batch_size': 256, 'n_clusters': 10, 'module_name': 'Datasets.Datasets'}, 'emnist_mnist': {'batch_size': 1024, 'n_clusters': 10, 'module_name': 'Datasets.Datasets'}, 'emnist_balanced_digits': {'batch_size': 256, 'n_clusters': 10, 'module_name': 'Datasets.Datasets'}, 'dermatology': {'batch_size': 64, 'n_clusters': 6, 'module_name': 'Datasets.Datasets'}, 'ecoil': {'batch_size': 64, 'n_clusters': 8, 'module_name': 'Datasets.Datasets'}, 'iris': {'batch_size': 64, 'n_clusters': 3, 'module_name': 'Datasets.Datasets'}, 'rings': {'batch_size': 64, 'n_clusters': 2, 'module_name': 'Datasets.Datasets'}, 'wine': {'batch_size': 64, 'n_clusters': 3, 'module_name': 'Datasets.Datasets'}, 'australian': {'batch_size': 64, 'n_clusters': 2, 'module_name': 'Datasets.Datasets'}, 'moons': {'batch_size': 64, 'n_clusters': 2, 'module_name': 'Datasets.Datasets'}, 'squeezed_gauss': {'batch_size': 64, 'n_clusters': 2, 'module_name': 'Datasets.Datasets'}, 'gauss_densities': {'batch_size': 64, 'n_clusters': 3, 'module_name': 'Datasets.Datasets'}, 'pendigits': {'batch_size': 256, 'n_clusters': 10, 'module_name': 'Datasets.Datasets'}, 'fashionmnist': {'batch_size': 1024, 'n_clusters': 10, 'module_name': 'Datasets.Datasets'}, '3dspheres': {'batch_size': 256, 'n_clusters': 2, 'module_name': 'Datasets.Datasets'}, '20_newsgroups': {'batch_size': 256, 'n_clusters': 10, 'module_name': 'Datasets.Datasets'}, 'coil20': {'batch_size': 200, 'n_clusters': 20, 'module_name': 'Datasets.Image_Datasets'}, 'cifar10': {'batch_size': 1024, 'n_clusters': 10, 'module_name': 'Datasets.Image_Datasets'}, 'stl10': {'batch_size': 1024, 'n_clusters': 10, 'module_name': 'Datasets.Image_Datasets'}, 'r15': {'batch_size': 50, 'n_clusters': 15, 'module_name': 'Datasets.Datasets'}, 'r3': {'batch_size': 100, 'n_clusters': 3, 'module_name': 'Datasets.Datasets'}, 'olivetti_faces': {'batch_size': 32, 'n_clusters': 40, 'module_name': 'Datasets.Datasets'}, 'r100': {'batch_size': 1024, 'n_clusters': 4, 'module_name': 'Datasets.Datasets'}, '10x73k': {'batch_size': 1024, 'n_clusters': 8, 'module_name': 'Datasets.Datasets'}, 'usps': {'batch_size': 256, 'n_clusters': 10, 'module_name': 'Datasets.Datasets'}, 'kmnist': {'batch_size': 1024, 'n_clusters': 10, 'module_name': 'Datasets.Datasets'}, 'reuters_4': {'batch_size': 1024, 'n_clusters': 4, 'module_name': 'Datasets.Datasets'}, 'hars': {'batch_size': 256, 'n_clusters': 6, 'module_name': 'Datasets.Datasets'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\PAPASOFT\n",
      "[nltk_data]     INC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\PAPASOFT\n",
      "[nltk_data]     INC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "hashmap_path = path_to_module + 'Datasets/'\n",
    "hashmap = get_hashmap(hashmap_path)\n",
    "print(hashmap)\n",
    "dataset_name = \"emnist_balanced_letters\"\n",
    "dataset_properties = hashmap[dataset_name]\n",
    "batch_size = dataset_properties['batch_size'] = 256\n",
    "n_clusters = dataset_properties['n_clusters'] #= 4\n",
    "dataloader, input_dim, data_np, labels = function_get_dataset(dataset_name, dataset_properties)\n",
    "print(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91742e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization = Visualization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8e3e04",
   "metadata": {},
   "source": [
    "### Dcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93a39599",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 19.117621\n",
      "Epoch: 1, Loss: 6.657898\n",
      "Epoch: 2, Loss: 5.376824\n",
      "Epoch: 3, Loss: 4.595087\n",
      "Epoch: 4, Loss: 4.103640\n",
      "Epoch: 5, Loss: 3.782311\n",
      "Epoch: 6, Loss: 3.557128\n",
      "Epoch: 7, Loss: 3.337781\n",
      "Epoch: 8, Loss: 3.200658\n",
      "Epoch: 9, Loss: 3.064718\n",
      "Epoch: 10, Loss: 2.966107\n",
      "Epoch: 11, Loss: 2.870135\n",
      "Epoch: 12, Loss: 2.774955\n",
      "Epoch: 13, Loss: 2.712431\n",
      "Epoch: 14, Loss: 2.649657\n",
      "Epoch: 15, Loss: 2.586892\n",
      "Epoch: 16, Loss: 2.552765\n",
      "Epoch: 17, Loss: 2.500370\n",
      "Epoch: 18, Loss: 2.438231\n",
      "Epoch: 19, Loss: 2.426848\n",
      "Epoch: 20, Loss: 2.389579\n",
      "Epoch: 21, Loss: 2.346014\n",
      "Epoch: 22, Loss: 2.318884\n",
      "Epoch: 23, Loss: 2.282825\n",
      "Epoch: 24, Loss: 2.245002\n",
      "Epoch: 25, Loss: 2.225906\n",
      "Epoch: 26, Loss: 2.212159\n",
      "Epoch: 27, Loss: 2.193102\n",
      "Epoch: 28, Loss: 2.167720\n",
      "Epoch: 29, Loss: 2.133860\n",
      "Epoch: 30, Loss: 2.121269\n",
      "Epoch: 31, Loss: 2.109725\n",
      "Epoch: 32, Loss: 2.086991\n",
      "Epoch: 33, Loss: 2.069735\n",
      "Epoch: 34, Loss: 2.038218\n",
      "Epoch: 35, Loss: 2.032554\n",
      "Epoch: 36, Loss: 2.015365\n",
      "Epoch: 37, Loss: 1.997417\n",
      "Epoch: 38, Loss: 1.983332\n",
      "Epoch: 39, Loss: 1.966625\n",
      "Epoch: 40, Loss: 1.943724\n",
      "Epoch: 41, Loss: 1.914582\n",
      "Epoch: 42, Loss: 1.916977\n",
      "Epoch: 43, Loss: 1.877226\n",
      "Epoch: 44, Loss: 1.892019\n",
      "Epoch: 45, Loss: 1.877549\n",
      "Epoch: 46, Loss: 1.853190\n",
      "Epoch: 47, Loss: 1.854869\n",
      "Epoch: 48, Loss: 1.851491\n",
      "Epoch: 49, Loss: 1.829589\n",
      "Epoch: 50, Loss: 1.828046\n",
      "Epoch: 51, Loss: 1.831938\n",
      "Epoch: 52, Loss: 1.795383\n",
      "Epoch: 53, Loss: 1.786350\n",
      "Epoch: 54, Loss: 1.770236\n",
      "Epoch: 55, Loss: 1.744040\n",
      "Epoch: 56, Loss: 1.757795\n",
      "Epoch: 57, Loss: 1.755823\n",
      "Epoch: 58, Loss: 1.754324\n",
      "Epoch: 59, Loss: 1.738900\n",
      "Epoch: 60, Loss: 1.713220\n",
      "Epoch: 61, Loss: 1.707418\n",
      "Epoch: 62, Loss: 1.721736\n",
      "Epoch: 63, Loss: 1.696779\n",
      "Epoch: 64, Loss: 1.692764\n",
      "Epoch: 65, Loss: 1.683342\n",
      "Epoch: 66, Loss: 1.687345\n",
      "Epoch: 67, Loss: 1.688412\n",
      "Epoch: 68, Loss: 1.656638\n",
      "Epoch: 69, Loss: 1.658390\n",
      "Epoch: 70, Loss: 1.673504\n",
      "Epoch: 71, Loss: 1.662852\n",
      "Epoch: 72, Loss: 1.647521\n",
      "Epoch: 73, Loss: 1.652680\n",
      "Epoch: 74, Loss: 1.647219\n",
      "Epoch: 75, Loss: 1.629697\n",
      "Epoch: 76, Loss: 1.632304\n",
      "Epoch: 77, Loss: 1.612313\n",
      "Epoch: 78, Loss: 1.613436\n",
      "Epoch: 79, Loss: 1.600445\n",
      "Epoch: 80, Loss: 1.617048\n",
      "Epoch: 81, Loss: 1.599180\n",
      "Epoch: 82, Loss: 1.593154\n",
      "Epoch: 83, Loss: 1.604496\n",
      "Epoch: 84, Loss: 1.584592\n",
      "Epoch: 85, Loss: 1.573318\n",
      "Epoch: 86, Loss: 1.577892\n",
      "Epoch: 87, Loss: 1.574457\n",
      "Epoch: 88, Loss: 1.574377\n",
      "Epoch: 89, Loss: 1.563135\n",
      "Epoch: 90, Loss: 1.571724\n",
      "Epoch: 91, Loss: 1.561293\n",
      "Epoch: 92, Loss: 1.574715\n",
      "Epoch: 93, Loss: 1.557173\n",
      "Epoch: 94, Loss: 1.557152\n",
      "Epoch: 95, Loss: 1.551977\n",
      "Epoch: 96, Loss: 1.528264\n",
      "Epoch: 97, Loss: 1.532835\n",
      "Epoch: 98, Loss: 1.516858\n",
      "Epoch: 99, Loss: 1.528849\n",
      "Directory './Results/emnist_balanced_letters/DCNAE/100_Eps_ld_10_out_10_bs_256_lr_0.0005_lamda_1_beta_1e-05/Weigths' already exists.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "parser.add_argument('--n-classes', type=int, default=10,\n",
    "                    help='output dimension')\n",
    "\"\"\"\n",
    "n_clusters = dataset_properties['n_clusters'] #= 4\n",
    "\n",
    "# Batch Size and Number of Clusters\n",
    "batch_size = dataset_properties['batch_size'] #= 256\n",
    "\n",
    "# Pre-Training Epochs and Learning Rate\n",
    "\"\"\"\n",
    "parser.add_argument('--pre-epoch', type=int, default=50, \n",
    "                    help='number of pre-train epochs')\n",
    "\"\"\"\n",
    "n_pret_epochs = 100\n",
    "pret_lr = 5e-4\n",
    "\n",
    "# Lamdas, Training Epochs and Learning Rate\n",
    "\"\"\"\n",
    "parser.add_argument('--epoch', type=int, default=100,\n",
    "                    help='number of epochs to train')\n",
    "\"\"\"\n",
    "n_epochs = 100\n",
    "\"\"\"\n",
    "parser.add_argument('--lr', type=float, default=1e-4,\n",
    "                    help='learning rate (default: 1e-4)')\n",
    "\"\"\"\n",
    "lr = 5e-4\n",
    "\"\"\"\n",
    "parser.add_argument('--lamda', type=float, default=1,\n",
    "                    help='coefficient of the reconstruction loss')\n",
    "\"\"\"\n",
    "lamda = 1\n",
    "\"\"\"\n",
    "parser.add_argument('--beta', type=float, default=1,\n",
    "                    help=('coefficient of the regularization term on '\n",
    "                          'clustering'))\n",
    "\"\"\"\n",
    "beta = 1e-5\n",
    "\"\"\"\n",
    "parser.add_argument('--latent_dim', type=int, default=10,\n",
    "                    help='latent space dimension')\n",
    "\"\"\"\n",
    "negative_slope = 0\n",
    "latent_dim = 10\n",
    "n_channels = 1\n",
    "is_MLP_AE = False\n",
    "\n",
    "if(is_MLP_AE):\n",
    "    dcn_autoencoder = DCNAutoencoder(device=device, n_clusters=n_clusters, input_dim=input_dim, latent_dim=latent_dim, negative_slope=negative_slope)\n",
    "else:  \n",
    "    input_dim = 1\n",
    "    dcn_autoencoder = DCNCDAutoencoder(device=device, n_clusters=n_clusters, input_dim=input_dim, latent_dim=latent_dim, negative_slope=negative_slope, n_channels=n_channels) \n",
    "dcn_autoencoder.set_general_training_variables(dataloader=dataloader, batch_size=batch_size)\n",
    "dcn_autoencoder.set_pretraining_variables(n_pret_epochs=n_pret_epochs, pret_lr=pret_lr)\n",
    "dcn_autoencoder.set_training_variables(n_epochs=n_epochs, lr=lr, lamda=lamda, beta=beta )\n",
    "dcn_autoencoder.set_path_variables(path_to_module=path_to_module, dataset_name=dataset_name)\n",
    "dcn_autoencoder.set_path()\n",
    "dcn_autoencoder = dcn_autoencoder.to(device)\n",
    "dcn_autoencoder\n",
    "pretrain = True\n",
    "if(pretrain):\n",
    "    dcn_autoencoder.pretrain_autoencoder()\n",
    "    dcn_autoencoder.save_pretrained_weights()\n",
    "else:\n",
    "    model_save_path = dcn_autoencoder.data_dir_path + '/Weigths/autoencoder_weights.pth'\n",
    "    dcn_autoencoder.load_state_dict(torch.load(model_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0fcf5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep: 0 L: 2.0214 Rec L: 1.5869 Dist L: 0.4345 ACC: 0.73 PUR: 0.77 NMI: 0.71 ARI: 0.61\n",
      "Ep: 1 L: 1.9445 Rec L: 1.5905 Dist L: 0.3540 ACC: 0.73 PUR: 0.78 NMI: 0.72 ARI: 0.62\n",
      "Ep: 2 L: 1.8851 Rec L: 1.5720 Dist L: 0.3131 ACC: 0.73 PUR: 0.78 NMI: 0.72 ARI: 0.62\n",
      "Ep: 3 L: 1.8534 Rec L: 1.5700 Dist L: 0.2835 ACC: 0.74 PUR: 0.78 NMI: 0.73 ARI: 0.63\n",
      "Ep: 4 L: 1.8181 Rec L: 1.5575 Dist L: 0.2607 ACC: 0.74 PUR: 0.78 NMI: 0.73 ARI: 0.63\n",
      "Ep: 5 L: 1.8009 Rec L: 1.5571 Dist L: 0.2438 ACC: 0.74 PUR: 0.79 NMI: 0.73 ARI: 0.64\n",
      "Ep: 6 L: 1.7971 Rec L: 1.5659 Dist L: 0.2312 ACC: 0.74 PUR: 0.79 NMI: 0.74 ARI: 0.64\n",
      "Ep: 7 L: 1.7673 Rec L: 1.5481 Dist L: 0.2192 ACC: 0.74 PUR: 0.79 NMI: 0.74 ARI: 0.64\n",
      "Ep: 8 L: 1.7888 Rec L: 1.5776 Dist L: 0.2112 ACC: 0.75 PUR: 0.79 NMI: 0.74 ARI: 0.64\n",
      "Ep: 9 L: 1.7440 Rec L: 1.5431 Dist L: 0.2008 ACC: 0.75 PUR: 0.79 NMI: 0.74 ARI: 0.64\n",
      "Ep: 10 L: 1.7394 Rec L: 1.5463 Dist L: 0.1931 ACC: 0.75 PUR: 0.79 NMI: 0.74 ARI: 0.64\n",
      "Ep: 11 L: 1.7105 Rec L: 1.5254 Dist L: 0.1851 ACC: 0.75 PUR: 0.80 NMI: 0.74 ARI: 0.65\n",
      "Ep: 12 L: 1.7336 Rec L: 1.5528 Dist L: 0.1808 ACC: 0.75 PUR: 0.80 NMI: 0.74 ARI: 0.65\n",
      "Ep: 13 L: 1.7005 Rec L: 1.5257 Dist L: 0.1748 ACC: 0.75 PUR: 0.80 NMI: 0.74 ARI: 0.65\n",
      "Ep: 14 L: 1.7147 Rec L: 1.5444 Dist L: 0.1703 ACC: 0.75 PUR: 0.80 NMI: 0.74 ARI: 0.65\n",
      "Ep: 15 L: 1.7080 Rec L: 1.5413 Dist L: 0.1667 ACC: 0.75 PUR: 0.80 NMI: 0.74 ARI: 0.65\n",
      "Ep: 16 L: 1.6878 Rec L: 1.5263 Dist L: 0.1615 ACC: 0.75 PUR: 0.80 NMI: 0.75 ARI: 0.65\n",
      "Ep: 17 L: 1.7011 Rec L: 1.5416 Dist L: 0.1594 ACC: 0.75 PUR: 0.80 NMI: 0.75 ARI: 0.65\n",
      "Ep: 18 L: 1.6710 Rec L: 1.5169 Dist L: 0.1541 ACC: 0.75 PUR: 0.80 NMI: 0.75 ARI: 0.65\n",
      "Ep: 19 L: 1.6751 Rec L: 1.5239 Dist L: 0.1512 ACC: 0.75 PUR: 0.80 NMI: 0.75 ARI: 0.65\n",
      "Ep: 20 L: 1.6806 Rec L: 1.5322 Dist L: 0.1484 ACC: 0.75 PUR: 0.80 NMI: 0.75 ARI: 0.65\n",
      "Ep: 21 L: 1.6722 Rec L: 1.5253 Dist L: 0.1469 ACC: 0.75 PUR: 0.80 NMI: 0.75 ARI: 0.65\n",
      "Ep: 22 L: 1.6607 Rec L: 1.5171 Dist L: 0.1436 ACC: 0.75 PUR: 0.80 NMI: 0.75 ARI: 0.65\n",
      "Ep: 23 L: 1.6335 Rec L: 1.4928 Dist L: 0.1408 ACC: 0.75 PUR: 0.80 NMI: 0.75 ARI: 0.65\n",
      "Ep: 24 L: 1.6307 Rec L: 1.4926 Dist L: 0.1381 ACC: 0.75 PUR: 0.80 NMI: 0.75 ARI: 0.65\n",
      "Ep: 25 L: 1.6376 Rec L: 1.5022 Dist L: 0.1354 ACC: 0.75 PUR: 0.80 NMI: 0.75 ARI: 0.65\n",
      "Ep: 26 L: 1.6423 Rec L: 1.5086 Dist L: 0.1337 ACC: 0.75 PUR: 0.80 NMI: 0.75 ARI: 0.65\n",
      "Ep: 27 L: 1.6258 Rec L: 1.4934 Dist L: 0.1325 ACC: 0.75 PUR: 0.80 NMI: 0.75 ARI: 0.65\n",
      "Ep: 28 L: 1.6213 Rec L: 1.4915 Dist L: 0.1298 ACC: 0.75 PUR: 0.80 NMI: 0.75 ARI: 0.65\n",
      "Ep: 29 L: 1.6298 Rec L: 1.5015 Dist L: 0.1283 ACC: 0.76 PUR: 0.80 NMI: 0.75 ARI: 0.65\n",
      "Ep: 30 L: 1.6186 Rec L: 1.4924 Dist L: 0.1262 ACC: 0.76 PUR: 0.80 NMI: 0.75 ARI: 0.66\n",
      "Ep: 31 L: 1.6219 Rec L: 1.4958 Dist L: 0.1261 ACC: 0.76 PUR: 0.80 NMI: 0.75 ARI: 0.66\n",
      "Ep: 32 L: 1.5946 Rec L: 1.4709 Dist L: 0.1237 ACC: 0.76 PUR: 0.80 NMI: 0.75 ARI: 0.66\n",
      "Ep: 33 L: 1.5892 Rec L: 1.4683 Dist L: 0.1208 ACC: 0.76 PUR: 0.80 NMI: 0.75 ARI: 0.66\n",
      "Ep: 34 L: 1.5995 Rec L: 1.4781 Dist L: 0.1214 ACC: 0.76 PUR: 0.80 NMI: 0.75 ARI: 0.66\n",
      "Ep: 35 L: 1.6197 Rec L: 1.5003 Dist L: 0.1194 ACC: 0.76 PUR: 0.80 NMI: 0.75 ARI: 0.65\n",
      "Ep: 36 L: 1.5830 Rec L: 1.4649 Dist L: 0.1181 ACC: 0.76 PUR: 0.80 NMI: 0.75 ARI: 0.66\n",
      "Ep: 37 L: 1.5936 Rec L: 1.4762 Dist L: 0.1173 ACC: 0.76 PUR: 0.80 NMI: 0.76 ARI: 0.66\n",
      "Ep: 38 L: 1.5800 Rec L: 1.4649 Dist L: 0.1151 ACC: 0.76 PUR: 0.80 NMI: 0.75 ARI: 0.66\n",
      "Ep: 39 L: 1.5850 Rec L: 1.4712 Dist L: 0.1138 ACC: 0.76 PUR: 0.80 NMI: 0.76 ARI: 0.66\n",
      "Ep: 40 L: 1.5723 Rec L: 1.4594 Dist L: 0.1129 ACC: 0.76 PUR: 0.80 NMI: 0.76 ARI: 0.66\n",
      "Ep: 41 L: 1.5706 Rec L: 1.4590 Dist L: 0.1117 ACC: 0.76 PUR: 0.80 NMI: 0.76 ARI: 0.66\n",
      "Ep: 42 L: 1.5938 Rec L: 1.4817 Dist L: 0.1121 ACC: 0.76 PUR: 0.80 NMI: 0.76 ARI: 0.66\n",
      "Ep: 43 L: 1.5756 Rec L: 1.4653 Dist L: 0.1103 ACC: 0.76 PUR: 0.80 NMI: 0.76 ARI: 0.66\n",
      "Ep: 44 L: 1.5514 Rec L: 1.4430 Dist L: 0.1084 ACC: 0.76 PUR: 0.80 NMI: 0.76 ARI: 0.66\n",
      "Ep: 45 L: 1.5566 Rec L: 1.4498 Dist L: 0.1069 ACC: 0.76 PUR: 0.81 NMI: 0.76 ARI: 0.66\n",
      "Ep: 46 L: 1.5750 Rec L: 1.4676 Dist L: 0.1075 ACC: 0.76 PUR: 0.80 NMI: 0.76 ARI: 0.66\n",
      "Ep: 47 L: 1.5667 Rec L: 1.4596 Dist L: 0.1072 ACC: 0.76 PUR: 0.81 NMI: 0.76 ARI: 0.66\n",
      "Ep: 48 L: 1.5760 Rec L: 1.4693 Dist L: 0.1067 ACC: 0.76 PUR: 0.80 NMI: 0.76 ARI: 0.66\n",
      "Ep: 49 L: 1.5518 Rec L: 1.4469 Dist L: 0.1049 ACC: 0.76 PUR: 0.81 NMI: 0.76 ARI: 0.66\n",
      "Ep: 50 L: 1.5589 Rec L: 1.4543 Dist L: 0.1046 ACC: 0.76 PUR: 0.81 NMI: 0.76 ARI: 0.66\n",
      "Ep: 51 L: 1.5526 Rec L: 1.4496 Dist L: 0.1030 ACC: 0.76 PUR: 0.81 NMI: 0.76 ARI: 0.66\n",
      "Ep: 52 L: 1.5648 Rec L: 1.4616 Dist L: 0.1032 ACC: 0.76 PUR: 0.80 NMI: 0.76 ARI: 0.66\n",
      "Ep: 53 L: 1.5449 Rec L: 1.4440 Dist L: 0.1009 ACC: 0.76 PUR: 0.81 NMI: 0.76 ARI: 0.66\n",
      "Ep: 54 L: 1.5301 Rec L: 1.4299 Dist L: 0.1002 ACC: 0.76 PUR: 0.81 NMI: 0.76 ARI: 0.66\n",
      "Ep: 55 L: 1.5636 Rec L: 1.4624 Dist L: 0.1011 ACC: 0.76 PUR: 0.80 NMI: 0.76 ARI: 0.66\n",
      "Ep: 56 L: 1.5408 Rec L: 1.4407 Dist L: 0.1002 ACC: 0.76 PUR: 0.80 NMI: 0.76 ARI: 0.66\n",
      "Ep: 57 L: 1.5401 Rec L: 1.4410 Dist L: 0.0992 ACC: 0.76 PUR: 0.80 NMI: 0.76 ARI: 0.66\n",
      "Ep: 58 L: 1.5285 Rec L: 1.4303 Dist L: 0.0982 ACC: 0.76 PUR: 0.81 NMI: 0.76 ARI: 0.66\n",
      "Ep: 59 L: 1.5466 Rec L: 1.4479 Dist L: 0.0987 ACC: 0.76 PUR: 0.81 NMI: 0.76 ARI: 0.66\n",
      "Ep: 60 L: 1.5320 Rec L: 1.4343 Dist L: 0.0977 ACC: 0.76 PUR: 0.81 NMI: 0.76 ARI: 0.66\n",
      "Ep: 61 L: 1.5324 Rec L: 1.4354 Dist L: 0.0970 ACC: 0.76 PUR: 0.81 NMI: 0.76 ARI: 0.66\n",
      "Ep: 62 L: 1.5230 Rec L: 1.4267 Dist L: 0.0963 ACC: 0.76 PUR: 0.81 NMI: 0.76 ARI: 0.66\n",
      "Ep: 63 L: 1.5245 Rec L: 1.4285 Dist L: 0.0960 ACC: 0.76 PUR: 0.81 NMI: 0.76 ARI: 0.66\n",
      "Ep: 64 L: 1.5297 Rec L: 1.4348 Dist L: 0.0950 ACC: 0.76 PUR: 0.81 NMI: 0.76 ARI: 0.66\n",
      "Ep: 65 L: 1.5130 Rec L: 1.4187 Dist L: 0.0943 ACC: 0.76 PUR: 0.80 NMI: 0.76 ARI: 0.66\n",
      "Ep: 66 L: 1.5153 Rec L: 1.4218 Dist L: 0.0935 ACC: 0.76 PUR: 0.81 NMI: 0.76 ARI: 0.66\n",
      "Ep: 67 L: 1.5157 Rec L: 1.4225 Dist L: 0.0932 ACC: 0.76 PUR: 0.81 NMI: 0.76 ARI: 0.66\n",
      "Ep: 68 L: 1.5323 Rec L: 1.4392 Dist L: 0.0931 ACC: 0.76 PUR: 0.80 NMI: 0.76 ARI: 0.66\n",
      "Ep: 69 L: 1.5154 Rec L: 1.4228 Dist L: 0.0925 ACC: 0.76 PUR: 0.80 NMI: 0.76 ARI: 0.66\n",
      "Ep: 70 L: 1.5310 Rec L: 1.4385 Dist L: 0.0925 ACC: 0.76 PUR: 0.81 NMI: 0.76 ARI: 0.66\n",
      "Ep: 71 L: 1.5210 Rec L: 1.4296 Dist L: 0.0914 ACC: 0.76 PUR: 0.80 NMI: 0.76 ARI: 0.66\n",
      "Ep: 72 L: 1.5088 Rec L: 1.4183 Dist L: 0.0905 ACC: 0.76 PUR: 0.81 NMI: 0.76 ARI: 0.66\n",
      "Ep: 73 L: 1.5032 Rec L: 1.4132 Dist L: 0.0900 ACC: 0.76 PUR: 0.80 NMI: 0.76 ARI: 0.66\n",
      "Ep: 74 L: 1.5022 Rec L: 1.4127 Dist L: 0.0895 ACC: 0.77 PUR: 0.81 NMI: 0.76 ARI: 0.66\n",
      "Ep: 75 L: 1.4963 Rec L: 1.4073 Dist L: 0.0890 ACC: 0.76 PUR: 0.80 NMI: 0.76 ARI: 0.66\n",
      "Ep: 76 L: 1.5038 Rec L: 1.4140 Dist L: 0.0898 ACC: 0.76 PUR: 0.81 NMI: 0.76 ARI: 0.66\n",
      "Ep: 77 L: 1.4837 Rec L: 1.3956 Dist L: 0.0881 ACC: 0.77 PUR: 0.81 NMI: 0.76 ARI: 0.66\n",
      "Ep: 78 L: 1.4907 Rec L: 1.4026 Dist L: 0.0880 ACC: 0.76 PUR: 0.80 NMI: 0.76 ARI: 0.66\n",
      "Ep: 79 L: 1.4935 Rec L: 1.4061 Dist L: 0.0874 ACC: 0.77 PUR: 0.81 NMI: 0.76 ARI: 0.67\n",
      "Ep: 80 L: 1.4873 Rec L: 1.4001 Dist L: 0.0872 ACC: 0.77 PUR: 0.81 NMI: 0.76 ARI: 0.66\n",
      "Ep: 81 L: 1.4880 Rec L: 1.4015 Dist L: 0.0865 ACC: 0.77 PUR: 0.81 NMI: 0.76 ARI: 0.66\n"
     ]
    }
   ],
   "source": [
    "dcn_autoencoder.train_autoencoder()\n",
    "cluster_centers = dcn_autoencoder.get_cluster_centers()\n",
    "_, dcn_autoencoder_reduced_data, labels = dcn_autoencoder.get_latent_data()\n",
    "General_Functions().save_excel(dcn_autoencoder.data_dir_path, dcn_autoencoder.df_eval)\n",
    "visualization.plot_tsne(dcn_autoencoder_reduced_data, labels, labels, cluster_centers, dcn_autoencoder.data_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c653bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization.plot(dcn_autoencoder_reduced_data, labels, labels, cluster_centers, dcn_autoencoder.data_dir_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
